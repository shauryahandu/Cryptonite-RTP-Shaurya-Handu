{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73e5cca-9571-4441-bcd1-67e2a00112f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "DATA_DIR = r\"C:\\Users\\aparn\\Downloads\\wine+quality\"\n",
    "np.random.seed(42)\n",
    "if not os.path.isdir(DATA_DIR):\n",
    "    print(f\"Warning: {DATA_DIR} not found. Using current working directory: {os.getcwd()}\")\n",
    "else:\n",
    "    os.chdir(DATA_DIR)\n",
    "    print(\"Changed working directory to:\", os.getcwd())\n",
    "\n",
    "expected = [\"winequality-red.csv\", \"winequality-white.csv\"]\n",
    "for fn in expected:\n",
    "    if not os.path.exists(fn):\n",
    "        raise FileNotFoundError(f\"Expected '{fn}' in {os.getcwd()}\")\n",
    "\n",
    "red = pd.read_csv(\"winequality-red.csv\", sep=\";\")\n",
    "white = pd.read_csv(\"winequality-white.csv\", sep=\";\")\n",
    "red['type'] = 0\n",
    "white['type'] = 1\n",
    "data = pd.concat([red, white], ignore_index=True)\n",
    "\n",
    "X = data.drop(columns=['quality']).values.astype(np.float64)\n",
    "y = data['quality'].values.astype(np.float64).reshape(-1, 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, shuffle=True\n",
    ")\n",
    "\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "class MLP:\n",
    "    def __init__(self, layer_sizes, activations, seed=42):\n",
    "        assert len(layer_sizes)-1 == len(activations)\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activations = activations\n",
    "        self.params = {}\n",
    "        rng = np.random.RandomState(seed)\n",
    "        for i in range(len(layer_sizes)-1):\n",
    "            n_in = layer_sizes[i]\n",
    "            n_out = layer_sizes[i+1]\n",
    "            limit = np.sqrt(6 / (n_in + n_out))\n",
    "            W = rng.uniform(-limit, limit, size=(n_in, n_out))\n",
    "            b = np.zeros((1, n_out))\n",
    "            self.params[f\"W{i+1}\"] = W\n",
    "            self.params[f\"b{i+1}\"] = b\n",
    "\n",
    "    def _activation(self, x, name):\n",
    "        if name == 'relu':\n",
    "            return np.maximum(0, x)\n",
    "        elif name == 'tanh':\n",
    "            return np.tanh(x)\n",
    "        elif name == 'sigmoid':\n",
    "            return 1 / (1 + np.exp(-x))\n",
    "        elif name == 'linear':\n",
    "            return x\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation\")\n",
    "\n",
    "    def _activation_deriv(self, x, name):\n",
    "        if name == 'relu':\n",
    "            return (x > 0).astype(float)\n",
    "        elif name == 'tanh':\n",
    "            t = np.tanh(x); return 1 - t**2\n",
    "        elif name == 'sigmoid':\n",
    "            s = 1 / (1 + np.exp(-x)); return s * (1 - s)\n",
    "        elif name == 'linear':\n",
    "            return np.ones_like(x)\n",
    "        else:\n",
    "            raise ValueError(\"Unknown activation\")\n",
    "\n",
    "    def forward(self, X):\n",
    "        caches = {'zs': [], 'as': []}\n",
    "        a = X\n",
    "        caches['as'].append(a)\n",
    "        for i in range(len(self.layer_sizes)-1):\n",
    "            W = self.params[f\"W{i+1}\"]\n",
    "            b = self.params[f\"b{i+1}\"]\n",
    "            z = a.dot(W) + b\n",
    "            caches['zs'].append(z)\n",
    "            a = self._activation(z, self.activations[i])\n",
    "            caches['as'].append(a)\n",
    "        return a, caches\n",
    "\n",
    "    def compute_loss(self, y_pred, y_true):\n",
    "        return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "    def backward(self, caches, y_pred, y_true):\n",
    "        grads = {}\n",
    "        m = y_true.shape[0]\n",
    "        dA = (2.0 / m) * (y_pred - y_true)\n",
    "        for i in reversed(range(len(self.layer_sizes)-1)):\n",
    "            z = caches['zs'][i]\n",
    "            a_prev = caches['as'][i]\n",
    "            dz = dA * self._activation_deriv(z, self.activations[i])\n",
    "            dW = a_prev.T.dot(dz)\n",
    "            db = np.sum(dz, axis=0, keepdims=True)\n",
    "            grads[f\"dW{i+1}\"] = dW\n",
    "            grads[f\"db{i+1}\"] = db\n",
    "            W = self.params[f\"W{i+1}\"]\n",
    "            dA = dz.dot(W.T)\n",
    "        return grads\n",
    "\n",
    "    def update_params(self, grads, lr=0.001):\n",
    "        for i in range(1, len(self.layer_sizes)):\n",
    "            self.params[f\"W{i}\"] -= lr * grads[f\"dW{i}\"]\n",
    "            self.params[f\"b{i}\"] -= lr * grads[f\"db{i}\"]\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred, _ = self.forward(X)\n",
    "        return y_pred\n",
    "\n",
    "n_features = X_train_scaled.shape[1]\n",
    "layer_sizes = [n_features, 64, 32, 1]\n",
    "activations = ['relu', 'relu', 'linear']\n",
    "mlp = MLP(layer_sizes, activations, seed=42)\n",
    "\n",
    "epochs = 300\n",
    "batch_size = 64\n",
    "lr = 0.01\n",
    "\n",
    "train_losses, val_losses = [], []\n",
    "n_samples = X_train_scaled.shape[0]\n",
    "steps_per_epoch = int(np.ceil(n_samples / batch_size))\n",
    "\n",
    "for epoch in range(1, epochs+1):\n",
    "    perm = np.random.permutation(n_samples)\n",
    "    X_shuffled, y_shuffled = X_train_scaled[perm], y_train[perm]\n",
    "    epoch_loss = 0.0\n",
    "    for step in range(steps_per_epoch):\n",
    "        start = step * batch_size\n",
    "        end = start + batch_size\n",
    "        X_batch = X_shuffled[start:end]\n",
    "        y_batch = y_shuffled[start:end]\n",
    "        y_pred, caches = mlp.forward(X_batch)\n",
    "        loss = mlp.compute_loss(y_pred, y_batch)\n",
    "        epoch_loss += loss * X_batch.shape[0]\n",
    "        grads = mlp.backward(caches, y_pred, y_batch)\n",
    "        mlp.update_params(grads, lr=lr)\n",
    "    epoch_loss /= n_samples\n",
    "    y_val_pred = mlp.predict(X_test_scaled)\n",
    "    val_loss = mlp.compute_loss(y_val_pred, y_test)\n",
    "    train_losses.append(epoch_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    if epoch % 25 == 0 or epoch in (1, epochs):\n",
    "        print(f\"Epoch {epoch:3d}/{epochs}  Train MSE: {epoch_loss:.4f}  Val MSE: {val_loss:.4f}\")\n",
    "\n",
    "y_test_pred = mlp.predict(X_test_scaled)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(\"\\nFinal evaluation on test set:\")\n",
    "print(f\"Test MSE: {mse_test:.4f}\")\n",
    "print(f\"Test RMSE: {rmse_test:.4f}\")\n",
    "print(f\"Test R^2: {r2:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(9,5))\n",
    "plt.plot(train_losses, label=\"Train\")\n",
    "plt.plot(val_losses, label=\"Val\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"mlp_loss_curve.png\")\n",
    "plt.show()\n",
    "print(\"Saved: mlp_loss_curve.png\")\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(y_test, y_test_pred, marker='x', alpha=0.6)\n",
    "plt.xlabel(\"Actual Quality\")\n",
    "plt.ylabel(\"Predicted Quality\")\n",
    "plt.title(\"Predicted vs Actual (test set)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"mlp_pred_vs_actual.png\")\n",
    "plt.show()\n",
    "print(\"Saved: mlp_pred_vs_actual.png\")\n",
    "\n",
    "np.savez(\"mlp_params.npz\", **mlp.params)\n",
    "pred_df = pd.DataFrame({\"actual\": y_test.ravel(), \"predicted\": y_test_pred.ravel()})\n",
    "print(\"\\nSample predictions (first 10):\")\n",
    "print(pred_df.head(10).to_string(index=False))\n",
    "print(\"\\nSaved model parameters to mlp_params.npz\")\n",
    "\n",
    "print(\"\\nArtifacts present in\", os.getcwd())\n",
    "for f in [\"mlp_loss_curve.png\", \"mlp_pred_vs_actual.png\", \"mlp_params.npz\"]:\n",
    "    print(\" -\", f, os.path.exists(f))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caeb41d5-edce-444a-9471-77cee16249c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381f15c3-7b28-41fd-a416-af6d7a7f2b95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbeb6b82-5415-4d26-af08-30e0c9830851",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
