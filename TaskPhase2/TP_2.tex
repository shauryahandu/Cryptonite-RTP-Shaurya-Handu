\documentclass[a4paper,11pt]{article}
\usepackage{amsmath,amssymb,graphicx,geometry,listings,xcolor,float,hyperref}
\geometry{margin=1in}
\setlength{\parskip}{0.8em}
\setlength{\parindent}{0pt}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=cyan
}

\definecolor{lightgray}{gray}{0.95}
\lstset{
  backgroundcolor=\color{lightgray},
  basicstyle=\ttfamily\small,
  breaklines=true,
  frame=single,
  keywordstyle=\color{blue},
  commentstyle=\color{green!50!black},
  stringstyle=\color{red},
  showstringspaces=false
}

\begin{document}

\begin{center}
{\Large \textbf{Task-2 Detailed Report}}\\[0.3cm]
\textbf{Fraud Detection (Decision Tree from Scratch) \& Pokémon DBSCAN Clustering}\\
\vspace{0.2cm}
\textit{A Detailed Technical Report}
\end{center}

\section{Introduction}
This report documents two machine learning tasks implemented \textbf{from scratch}:\\
(1) Fraud Detection using a custom Decision Tree classifier, and\\
(2) Unsupervised clustering of Pokémon data using PCA + DBSCAN.

The objective of this assignment was to avoid black-box ML libraries and instead implement core ML algorithms manually, gaining deeper understanding of the mathematical foundations behind them.

This report includes:
\begin{itemize}
    \item Mathematical explanation of Decision Trees, PCA, and DBSCAN
    \item Python code snippets of the scratch implementations
    \item Evaluation results and visual outputs
    \item Comparison with Scikit-Learn equivalents
    \item Research-style discussion and observations
\end{itemize}

% =======================================================================
\section{Part 1: Fraud Detection using a Scratch Decision Tree}

\subsection{Dataset Overview and Preprocessing}
The dataset \texttt{onlinefraud.csv} contains financial transaction records labelled as fraud or non-fraud. The following preprocessing steps were performed:
\begin{itemize}
    \item Removal of irrelevant or constant-value columns
    \item Median imputation for missing numeric values
    \item Factorization for categorical variables
    \item Undersampling to balance the fraud and non-fraud classes
\end{itemize}

\subsection{Mathematics of Decision Trees (Gini Impurity)}

A Decision Tree recursively splits the data using a feature and threshold that maximizes class separation.

\subsubsection*{Gini Impurity}
Gini impurity measures how impure a node is:

\begin{equation}
Gini = 1 - \sum_{i=1}^{C} p_i^2
\end{equation}

where $p_i$ is the proportion of class $i$ in the node.

\subsubsection*{Information Gain}
A split is chosen to maximize the reduction in impurity:

\begin{equation}
Gain = Gini_{parent} - \left( 
\frac{N_L}{N} Gini_L + 
\frac{N_R}{N} Gini_R \right)
\end{equation}

\subsection{Scratch Code Snippet}
Below is a simplified version of the code used to compute Gini and determine the best split:

\begin{lstlisting}[language=Python, caption=Gini + Best Split Implementation]
def gini(self, y):
    counts = np.bincount(y)
    probs = counts / len(y)
    return 1.0 - np.sum(probs ** 2)

def best_split(self, X, y):
    parent = self.gini(y)
    best_gain = 0
    for feat in range(X.shape[1]):
        for t in np.unique(X[:, feat]):
            left = X[:, feat] <= t
            right = ~left
            gain = compute_gain(...)
            if gain > best_gain:
                best_feat, best_thresh = feat, t
\end{lstlisting}

\subsection{Model Evaluation Results}
The model was trained using an 80-20 split. Evaluation metrics computed:

\begin{itemize}
    \item Confusion Matrix
    \item Precision, Recall, F1-score
    \item ROC Curve and AUC
    \item Feature Importance based on Information Gain
\end{itemize}

\textbf{Plots to Insert in Report:}
\begin{itemize}
    \item Confusion Matrix Heatmap
    \item ROC Curve
    \item Feature Importance Bar Graph
\end{itemize}

\subsection{Comparison with Scikit-Learn Decision Tree}
The scratch decision tree was compared with the Scikit-Learn implementation:

\begin{itemize}
    \item Scratch model produced comparable accuracy and AUC
    \item Differences arise due to:
    \begin{itemize}
        \item No pruning in scratch model
        \item Threshold search limited to unique feature values
        \item Scikit-Learn uses optimized C-based computations
    \end{itemize}
\end{itemize}

% =======================================================================
\section{Part 2: Pokémon Clustering using PCA + DBSCAN}

\subsection{Dimensionality Reduction with PCA}
The Pokémon dataset contains stats and categorical attributes (types, abilities). One-hot encoding results in a high-dimensional feature space.

To reduce dimensionality while preserving variance, PCA projects data into a lower-dimensional space:

\begin{equation}
X_{pca} = (X - \mu) W_k^T
\end{equation}

where $W_k$ contains the top $k$ eigenvectors of the covariance matrix.

20 components were used, retaining most of the variance.

\subsection{DBSCAN Clustering}
DBSCAN groups points based on density. It requires two parameters:

\begin{itemize}
    \item $\epsilon$ (eps): neighborhood radius
    \item $minPts$: minimum points to form a dense region
\end{itemize}

A core point satisfies:

\begin{equation}
|N_\epsilon(p)| \geq minPts
\end{equation}

Points not belonging to any cluster are labeled as noise.

Silhouette scores were computed for multiple values of eps and minPts.

\subsection{Scratch DBSCAN Implementation Snippet}
\begin{lstlisting}[language=Python, caption=Core DBSCAN Expansion Logic]
if len(neighbors) < min_samples:
    labels[i] = -1  # noise
else:
    labels[i] = cluster_id
    queue = list(neighbors)
    while queue:
        j = queue.pop(0)
        if labels[j] == -1:
            labels[j] = cluster_id
\end{lstlisting}

\subsection{Results and Visualization}
The outputs included:
\begin{itemize}
    \item DBSCAN cluster scatter plot
    \item Silhouette score vs eps graph
    \item Silhouette score vs minPts graph
    \item Final Pokémon dataset saved with cluster labels
\end{itemize}

% =======================================================================
\section{Conclusion}
Both machine learning algorithms were successfully implemented from scratch and performed similarly to their Scikit-Learn equivalents. PCA + DBSCAN revealed meaningful Pokémon groupings based on abilities and stats, while the scratch Decision Tree effectively classified fraud cases.

These implementations demonstrate a strong understanding of core ML mathematics, algorithm mechanics, and evaluation methodology.

\end{document}
