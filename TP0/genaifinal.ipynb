{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7993940,"sourceType":"datasetVersion","datasetId":4706096}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install sentence-transformers transformers faiss-cpu nltk --quiet\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-17T12:23:14.497531Z","iopub.execute_input":"2026-02-17T12:23:14.497739Z","iopub.status.idle":"2026-02-17T12:23:17.861250Z","shell.execute_reply.started":"2026-02-17T12:23:14.497718Z","shell.execute_reply":"2026-02-17T12:23:17.860516Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import re\nimport faiss\nimport nltk\nimport numpy as np\nimport torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nfrom sentence_transformers import SentenceTransformer\nfrom nltk.tokenize import sent_tokenize\n\nnltk.download(\"punkt\")\n\n# GPU setup\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device:\", device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T12:23:17.862587Z","iopub.execute_input":"2026-02-17T12:23:17.862956Z","iopub.status.idle":"2026-02-17T12:23:28.094074Z","shell.execute_reply.started":"2026-02-17T12:23:17.862913Z","shell.execute_reply":"2026-02-17T12:23:28.093342Z"}},"outputs":[{"name":"stderr","text":"2026-02-17 12:23:23.712062: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1771331003.734615     309 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1771331003.743659     309 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1771331003.760840     309 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771331003.760871     309 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771331003.760873     309 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1771331003.760875     309 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"with open(\"/kaggle/input/datasets/ffatty/plain-text-wikipedia-simpleenglish/AllCombined.txt\",\n          \"r\",\n          encoding=\"utf-8\") as f:\n    raw_documents = f.read().split(\"\\n\\n\")\n\nprint(\"Total documents:\", len(raw_documents))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T12:23:28.095151Z","iopub.execute_input":"2026-02-17T12:23:28.095887Z","iopub.status.idle":"2026-02-17T12:23:30.690586Z","shell.execute_reply.started":"2026-02-17T12:23:28.095848Z","shell.execute_reply":"2026-02-17T12:23:30.689707Z"}},"outputs":[{"name":"stdout","text":"Total documents: 968274\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"editor_model_name = \"google/flan-t5-small\"  # 80M parameters\n\ntokenizer = AutoTokenizer.from_pretrained(editor_model_name)\n\ndef chunk_text(text, min_tokens=300, max_tokens=600, stride=400):\n    tokens = tokenizer.encode(text, add_special_tokens=False)\n    chunks = []\n\n    for i in range(0, len(tokens), stride):\n        chunk_tokens = tokens[i:i+max_tokens]\n        if len(chunk_tokens) == 0:\n            continue\n        chunk_text = tokenizer.decode(chunk_tokens)\n        chunks.append(chunk_text)\n\n    return chunks\n\nchunks = []\nfor doc in raw_documents:\n    chunks.extend(chunk_text(doc))\n\nprint(\"Total chunks:\", len(chunks))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T12:23:30.691695Z","iopub.execute_input":"2026-02-17T12:23:30.692129Z","iopub.status.idle":"2026-02-17T12:26:41.206432Z","shell.execute_reply.started":"2026-02-17T12:23:30.692093Z","shell.execute_reply":"2026-02-17T12:26:41.205570Z"}},"outputs":[{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (584 > 512). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Total chunks: 934404\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"retriever = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n\nchunk_embeddings = retriever.encode(\n    chunks,\n    convert_to_numpy=True,\n    show_progress_bar=True\n)\n\ndimension = chunk_embeddings.shape[1]\nindex = faiss.IndexFlatIP(dimension)\n\nfaiss.normalize_L2(chunk_embeddings)\nindex.add(chunk_embeddings)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T12:26:41.207508Z","iopub.execute_input":"2026-02-17T12:26:41.207765Z","iopub.status.idle":"2026-02-17T12:36:39.247216Z","shell.execute_reply.started":"2026-02-17T12:26:41.207735Z","shell.execute_reply":"2026-02-17T12:36:39.246377Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/29201 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c13c316bca74f589f11d9fc90768b1c"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"model = AutoModelForSeq2SeqLM.from_pretrained(editor_model_name)\nmodel = model.to(device)\nmodel.eval()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T12:36:39.249623Z","iopub.execute_input":"2026-02-17T12:36:39.249871Z","iopub.status.idle":"2026-02-17T12:36:39.785431Z","shell.execute_reply.started":"2026-02-17T12:36:39.249849Z","shell.execute_reply":"2026-02-17T12:36:39.784823Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"T5ForConditionalGeneration(\n  (shared): Embedding(32128, 512)\n  (encoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-7): 7 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (decoder): T5Stack(\n    (embed_tokens): Embedding(32128, 512)\n    (block): ModuleList(\n      (0): T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n              (relative_attention_bias): Embedding(32, 6)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (1-7): 7 x T5Block(\n        (layer): ModuleList(\n          (0): T5LayerSelfAttention(\n            (SelfAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (1): T5LayerCrossAttention(\n            (EncDecAttention): T5Attention(\n              (q): Linear(in_features=512, out_features=384, bias=False)\n              (k): Linear(in_features=512, out_features=384, bias=False)\n              (v): Linear(in_features=512, out_features=384, bias=False)\n              (o): Linear(in_features=384, out_features=512, bias=False)\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (2): T5LayerFF(\n            (DenseReluDense): T5DenseGatedActDense(\n              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n              (wo): Linear(in_features=1024, out_features=512, bias=False)\n              (dropout): Dropout(p=0.1, inplace=False)\n              (act): NewGELUActivation()\n            )\n            (layer_norm): T5LayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (final_layer_norm): T5LayerNorm()\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"def retrieve(query, k=3):\n    k = min(k, 3)  # enforce k ≤ 3\n\n    query_embedding = retriever.encode([query], convert_to_numpy=True)\n    faiss.normalize_L2(query_embedding)\n\n    scores, indices = index.search(query_embedding, k)\n\n    return [chunks[i] for i in indices[0]], scores[0]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T12:36:39.786552Z","iopub.execute_input":"2026-02-17T12:36:39.786754Z","iopub.status.idle":"2026-02-17T12:36:39.791210Z","shell.execute_reply.started":"2026-02-17T12:36:39.786734Z","shell.execute_reply":"2026-02-17T12:36:39.790408Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def post_process(answer):\n    # Remove parentheses\n    answer = re.sub(r\"\\(.*?\\)\", \"\", answer)\n\n    # Remove extra whitespace\n    answer = re.sub(r\"\\s+\", \" \", answer).strip()\n\n    # Sentence limit (max 3)\n    sentences = sent_tokenize(answer)\n    sentences = sentences[:3]\n\n    # Merge very short sentences\n    merged = []\n    for s in sentences:\n        if len(s.split()) < 4 and merged:\n            merged[-1] += \" \" + s\n        else:\n            merged.append(s)\n\n    final = \" \".join(merged)\n\n    return final.strip()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T12:36:39.792110Z","iopub.execute_input":"2026-02-17T12:36:39.792349Z","iopub.status.idle":"2026-02-17T12:36:39.803121Z","shell.execute_reply.started":"2026-02-17T12:36:39.792326Z","shell.execute_reply":"2026-02-17T12:36:39.802557Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def answer_question(question):\n    retrieved_chunks, scores = retrieve(question, k=3)\n\n    # Fallback if similarity too low\n    if max(scores) < 0.2:\n        return \"Not enough information in the Simple Wikipedia dataset.\"\n\n    context = \"\\n\\n\".join(retrieved_chunks)\n\n    prompt = f\"\"\"\nAnswer the question using only the information below.\nUse simple English.\nWrite 2 to 3 sentences only.\nDo not copy text exactly. Rewrite it clearly.\n\nContext:\n{context}\n\nQuestion:\n{question}\n\"\"\"\n\n    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True)\n    inputs = {k: v.to(device) for k, v in inputs.items()}\n\n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_length=128,\n            do_sample=False,\n            temperature=0.0,\n            top_p=1.0,\n            num_beams=1\n        )\n\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n    return post_process(answer)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T12:36:39.804164Z","iopub.execute_input":"2026-02-17T12:36:39.804465Z","iopub.status.idle":"2026-02-17T12:36:39.819593Z","shell.execute_reply.started":"2026-02-17T12:36:39.804428Z","shell.execute_reply":"2026-02-17T12:36:39.818975Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"question = \"What is the Eiffel Tower?\"\nprint(answer_question(question))\nquestion1= \"Is India a good country?\"\nprint(answer_question(question1))\nquestion2= \"Are we better than Pakistan?\"\nprint(answer_question(question2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T13:03:42.595562Z","iopub.execute_input":"2026-02-17T13:03:42.595879Z","iopub.status.idle":"2026-02-17T13:03:43.943663Z","shell.execute_reply.started":"2026-02-17T13:03:42.595850Z","shell.execute_reply":"2026-02-17T13:03:43.942810Z"}},"outputs":[{"name":"stdout","text":"It was built in the style of the Eiffel Tower. It cost £45000 , is 518 ft tall and weighs 2586 tons. It is mainly made from steel and cast iron.\nIndia is a good country.\nYes, we are better than Pakistan.\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"import os\n\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-17T12:36:40.729802Z","iopub.status.idle":"2026-02-17T12:36:40.730092Z","shell.execute_reply.started":"2026-02-17T12:36:40.729942Z","shell.execute_reply":"2026-02-17T12:36:40.729958Z"}},"outputs":[],"execution_count":null}]}